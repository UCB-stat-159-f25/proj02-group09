### Contribution statement
John Jacobsen <jjacobsen@berkeley.edu> 
I did the text processing including SpaCy tokenization and lemmatization analysis, comparing tokens vs lemmas across 75,983 words. Developed the get_most_common_words() function for year-based word frequency analysis and created visualizations. Implemented TF-IDF vectorization on all 246 speeches with PCA dimensionality reduction, generating scatter plots and heatmaps for document-term relationships.

Marcos Negrete <marcos.negrete@berkeley.edu>
Implemented LDA topic modeling using both gensim and sklearn with the required 18 topics, displaying top 10 words per topic and topic distributions. Successfully implemented BERTopic with min_topic_size=3, created pyLDAvis visualizations, and generated all required outputs. Debugged and resolved environment configuration issues including scipy/gensim compatibility and kernel setup.

Evan Vlahos <evlahos@berkeley.edu>
Resolved git merge conflicts, ensured proper environment configuration with conda and SpaCy models, organized notebook structure with clear markdown headers, and saved all visualizations to the outputs folder for reproducibility.

Lillie Wang <lillie-wang@berkeley.edu> --> 
Created and Finished Part 1 
Created and add to contribution statement
Created and add to ai documentation
Created and add to output directory
Created binder and added to README
Created and add to MyST
Created Github Pages 
